{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1005 15:36:00.003447 140317477144320 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/enlr/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertPreTrainedModel, BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =  pd.read_pickle('train_processed.pkl')\n",
    "val_df =  pd.read_pickle('val_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_id = tokenizer.convert_tokens_to_ids(tokenizer._cls_token)\n",
    "sep_id = tokenizer.convert_tokens_to_ids(tokenizer._sep_token)\n",
    "\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def get_features_from_example(ex):\n",
    "    input = ex.input.copy()\n",
    "    pab = ex.pab_pos.copy()\n",
    "\n",
    "    #add special tokens [CLS at beginning], [SEP at end], [optional SEP before pos]\n",
    "    input = [cls_id]+input.tolist()+[sep_id]\n",
    "    pab += 1\n",
    "    \n",
    "    #attention masking and padding\n",
    "    mask = [1] * len(input)\n",
    "    pad_length = max_length -len(input)\n",
    "    #padding tokens and mask with 0\n",
    "    input = input + [0]*pad_length\n",
    "    mask = mask + [0]*pad_length\n",
    "\n",
    "    assert len(input) == max_length\n",
    "    assert len(mask) == max_length\n",
    "    \n",
    "    return input, mask, pab, int(ex.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    features = [get_features_from_example(df.iloc[i]) for i in range(len(df))]\n",
    "\n",
    "    ids = torch.tensor([feature[0] for feature in features])\n",
    "    masks = torch.tensor([feature[1] for feature in features])\n",
    "    pabs = torch.tensor([feature[2] for feature in features])\n",
    "    labels = torch.tensor([feature[3] for feature in features])\n",
    "\n",
    "    print(ids.size(), masks.size(), pabs.size(), labels.size())\n",
    "\n",
    "    return TensorDataset(ids, masks, pabs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 512]) torch.Size([2000, 512]) torch.Size([2000, 3]) torch.Size([2000])\n",
      "torch.Size([454, 512]) torch.Size([454, 512]) torch.Size([454, 3]) torch.Size([454])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_dataset(train_df)\n",
    "\n",
    "val_dataset = create_dataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "\n",
    "Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "    **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "        Classification (or regression if config.num_labels==1) loss.\n",
    "    **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "    **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "        list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "        of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "    **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "        list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "Examples::\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "\"\"\"\n",
    "class BertForPronousResolution(BertPreTrainedModel):\n",
    "    def __init__(self, config : BertConfig):\n",
    "        super(BertForPronousResolution, self).__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        #[P][A][B] classification layer\n",
    "        self.classification = torch.nn.Linear(config.hidden_size * 3 , 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pab, labels = None, token_type_ids = None ):\n",
    "   \n",
    "        output = self.bert(input_ids, attention_mask, token_type_ids, None, None)\n",
    "        last_hidden_states = output[0]\n",
    "        \n",
    "        print(last_hidden_states[0], last_hidden_states[0][pab[0]])\n",
    "        \n",
    "        batches = last_hidden_states.size()[0]\n",
    "        row_indexes = torch.arange(batches).unsqueeze(1) # row numbers in a column matrix\n",
    "        pab_hidden_states = last_hidden_states[row_indexes, pab] #batch size x 3 x hidden size\n",
    "        \n",
    "        concatenated_states = pab_hidden_states.view(batches,-1)\n",
    "        \n",
    "        print(concatenated_states)\n",
    "        logits = self.classification(concatenated_states)\n",
    "        \n",
    "        output = (logits,) + output[2:] #hidden states and attention if present\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fun = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fun(logits, labels)\n",
    "            \n",
    "            output = (loss,) + output\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1005 16:40:02.835144 140317477144320 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/enlr/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1005 16:40:02.836277 140317477144320 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1005 16:40:03.004810 140317477144320 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/enlr/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1005 16:40:06.312810 140317477144320 modeling_utils.py:405] Weights of BertForPronousResolution not initialized from pretrained model: ['classification.weight', 'classification.bias']\n",
      "I1005 16:40:06.313632 140317477144320 modeling_utils.py:408] Weights from pretrained model not used in BertForPronousResolution: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForPronousResolution.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[62, 42, 45]]), tensor([1]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = train_dataset[0:1]\n",
    "test_data[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8597, -0.3160, -0.1135,  ..., -0.1742,  0.5477,  0.5910],\n",
      "        [-0.4190,  0.3307,  0.3428,  ...,  0.0560,  0.2804, -0.2568],\n",
      "        [ 1.5684, -0.4883,  0.9586,  ...,  0.8987,  0.0998,  1.1028],\n",
      "        ...,\n",
      "        [-0.0892, -0.4428,  0.4997,  ...,  0.2574,  0.2867, -0.4949],\n",
      "        [ 0.0533, -0.3233,  0.1011,  ...,  0.0119,  0.0863, -0.2838],\n",
      "        [-0.2587, -0.4198,  0.3070,  ...,  0.1589,  0.2921, -0.5746]],\n",
      "       grad_fn=<SelectBackward>) tensor([[-0.0562, -1.0524,  0.1111,  ..., -0.2489,  0.2950, -0.1346],\n",
      "        [ 0.5295,  0.0873,  0.3644,  ..., -0.4559,  0.2341,  0.3187],\n",
      "        [ 0.3718,  0.0329,  1.0530,  ..., -0.1534,  0.0422,  0.3036]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "tensor([[-0.0562, -1.0524,  0.1111,  ..., -0.1534,  0.0422,  0.3036]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7031, grad_fn=<NllLossBackward>),\n",
       " tensor([[-0.5949,  0.2181, -0.3326]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_data[0],test_data[1], test_data[2],test_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
