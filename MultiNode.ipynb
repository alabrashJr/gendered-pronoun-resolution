{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 nodes 2 processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1 : Rank 0\n",
    "Not working Yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python TrainGAP.py \\\n",
    "--is_distributed \\\n",
    "--local_rank 0 \\\n",
    "--global_rank 0 \\\n",
    "--world_size 2 \\\n",
    "--master_node '<Master/Rank0 IP>' \\\n",
    "--master_port 29500 \\\n",
    "--backend nccl \\\n",
    "--per_gpu_batch_size 16 \\\n",
    "--gradient_accumulation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python TrainGAP.py \\\n",
    "# --is_distributed \\\n",
    "# --local_rank 0 \\\n",
    "# --global_rank 1 \\\n",
    "# --world_size 2 \\\n",
    "# --master_node '<Rank 0 IP>' \\\n",
    "# --master_port 29500 \\\n",
    "# --backend nccl \\\n",
    "# --per_gpu_batch_size 16 \\\n",
    "# --gradient_accumulation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common errors**\n",
    "\n",
    "1. Training does not progress or timeout error is seen\n",
    "\n",
    "    Follow this blog to test if distributed training is possible between the machines or not\n",
    "\n",
    "    https://krishansubudhi.github.io/deeplearning/2019/10/15/PyTorch-Distributed.html\n",
    "\n",
    "2. Address already in use\n",
    "\n",
    "    Same port was used for multi node communication which has not been released. \n",
    "\n",
    "    Restart kernel or kill any other distributed process which uses same port . \n",
    "\n",
    "3. Master node should always be rank 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 nodes 4 processes\n",
    "## using mp.spawn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python TrainGAP.py \\\n",
    "        --is_distributed \\\n",
    "        --world_size 4 \\\n",
    "        --nprocs 2 \\\n",
    "        --start_rank 0\\\n",
    "        --backend nccl \\\n",
    "        --master_node '13.84.41.106' \\\n",
    "        --master_port 29500 \\\n",
    "        --per_gpu_batch_size 8 \\\n",
    "        --gradient_accumulation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python TrainGAP.py \\\n",
    "#         --is_distributed \\\n",
    "#         --world_size 4 \\\n",
    "#         --nprocs 2 \\\n",
    "#         --start_rank 2\\\n",
    "#         --backend nccl \\\n",
    "#         --master_node '40.74.233.84' \\\n",
    "#         --master_port 29500 \\\n",
    "#         --per_gpu_batch_size 8 \\\n",
    "#         --gradient_accumulation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.distributed.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch \\\n",
    "    --nproc_per_node=2 \\\n",
    "    --nnodes=2 \\\n",
    "    --node_rank=0 \\\n",
    "    --master_addr='40.74.233.84' \\\n",
    "    --master_port=29500 \\\n",
    "    TrainGAP.py \\\n",
    "        --is_distributed \\\n",
    "        --per_gpu_batch_size 16 \\\n",
    "        --gradient_accumulation 4 \\\n",
    "        --backend nccl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m torch.distributed.launch \\\n",
    "#     --nproc_per_node=2 \\\n",
    "#     --nnodes=2 \\\n",
    "#     --node_rank=1 \\\n",
    "#     --master_addr='40.74.233.84' \\\n",
    "#     --master_port=29500 \\\n",
    "#     TrainGAP.py \\\n",
    "#         --is_distributed \\\n",
    "#         --per_gpu_batch_size 16 \\\n",
    "#         --gradient_accumulation 4 \\\n",
    "#         --backend nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
